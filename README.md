# DAG Complejo con Airflow: Pipeline Avanzado con Best Practices

## ðŸ“Œ Objetivo del Proyecto

Este proyecto tiene como objetivo implementar un **DAG complejo en Apache Airflow** que integre patrones avanzados de diseÃ±o, incluyendo:

- Branching dinÃ¡mico segÃºn la calidad de los datos.
- TaskGroups para agrupar tareas relacionadas.
- SubDAGs opcionales para bloques de tareas autÃ³nomos.
- Estrategias de escalado y buenas prÃ¡cticas de configuraciÃ³n.
- Testing automatizado de DAGs.
- IntegraciÃ³n de CI/CD usando GitHub Actions para asegurar la calidad del cÃ³digo antes de desplegar.

El propÃ³sito es **simular un pipeline de procesamiento de datos avanzado**, aplicando conceptos de ingenierÃ­a de datos y asegurando confiabilidad mediante pruebas y validaciÃ³n continua.

---

## ðŸ›  Estructura del Proyecto

```

dag_complejo/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ pipeline_complejo.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_dag_complejo.py
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ test-dags.yml
â””â”€â”€ README.md

````

---

## âš¡ Requisitos Previos

- Python 3.8+
- Apache Airflow 2.8.1
- pytest
- Git y GitHub (para CI/CD)

---

## ðŸš€ Pasos para Construir el Proyecto

### 1. Crear la carpeta del proyecto y entorno virtual
```bash
mkdir dag_complejo
cd dag_complejo
python3 -m venv myenv
source myenv/bin/activate  # Linux / Mac
# myenv\Scripts\activate    # Windows
````

### 2. Instalar Airflow

```bash
pip install "apache-airflow[webserver]==2.8.1" Flask-Session
```

### 3. Crear carpetas para DAGs y tests

```bash
mkdir dags tests
```

### 4. Crear el DAG complejo

* Archivo: `dags/pipeline_complejo.py`
* Implementa:

  * DummyOperators de inicio y fin
  * PythonOperators para validaciÃ³n y procesamiento
  * BranchPythonOperator para rutas condicionales
  * TaskGroup para procesamiento pesado
  * ConexiÃ³n y uniÃ³n de rutas al final

**Conceptos aplicados:**

* XComs para compartir informaciÃ³n entre tareas
* Branching dinÃ¡mico
* TaskGroup para agrupar tareas complejas
* Best practices: retries, timeout, schedule diario, catchup desactivado

---

### 5. Implementar Testing

* Archivo: `tests/test_dag_complejo.py`
* Incluye tests para:

  * Carga correcta del DAG
  * Dependencias correctas (upstream/downstream)
  * LÃ³gica de branching simulando contextos con `xcom_pull`
* Se utiliza **pytest** y fixtures para cargar DAGs

---

### 6. Configurar CI/CD con GitHub Actions

* Archivo: `.github/workflows/test-dags.yml`
* Funcionalidad:

  * Ejecutar tests automÃ¡ticamente al hacer push en `dags/**`
  * Instalar Python y dependencias
  * Ejecutar `pytest` para tests
  * Validar sintaxis y carga de todos los DAGs con `DagBag`
* Garantiza que cualquier cambio que rompa el DAG sea detectado antes de desplegarlo

---

### 7. Configurar Airflow para usar el DAG del proyecto

1. Definir el **AIRFLOW_HOME** apuntando a la carpeta del proyecto:

```bash
export AIRFLOW_HOME=$(pwd)  # Linux / Mac
# set AIRFLOW_HOME=%cd%      # Windows PowerShell
```

2. Inicializar la base de datos de Airflow (crea tablas y estructura necesaria):

# Migrar la base de datos (crea tablas si no existen y aplica migraciones)
airflow db migrate

# Aplicar migraciones pendientes para asegurar esquema actualizado
airflow db upgrade

3. Crear usuario administrador:

```bash
airflow users create \
    --username admin \
    --firstname Cristian \
    --lastname Iglesias \
    --role Admin \
    --email admin@example.com
```

4. Iniciar Scheduler y Webserver:

```bash
# Scheduler: ejecuta tareas programadas
airflow scheduler

# Webserver: interfaz UI de Airflow
airflow webserver --port 8080
```

5. Abrir UI en el navegador:

```
http://localhost:8080
```

Se recomiendo configurar el proyecto sin proyectos de ejemplo para que no haya conflictos con los DAGs del proyecto.
en airflow.cfg se recomienda configurar: 

```
include_examples = False
```

El DAG `pipeline_avanzado_complejo` deberÃ­a aparecer automÃ¡ticamente, cargando desde `dag_complejo/dags/pipeline_complejo.py`.

---

## âœ… Resumen de Conceptos Aplicados

* **Branching dinÃ¡mico:** decisiones segÃºn la calidad de los datos (`BranchPythonOperator`)
* **TaskGroup:** agrupa pasos complejos en un solo bloque visual
* **SubDAG (opcional):** Ãºtil para tareas independientes y escalables
* **Best Practices:** retries, timeout, catchup=False
* **Testing:** pytest, fixtures, pruebas unitarias y de dependencias
* **CI/CD:** GitHub Actions valida DAGs y tests automÃ¡ticamente

---

## ðŸ’¡ Buenas PrÃ¡cticas

1. Usar TaskGroups para organizar DAGs visualmente.
2. Reservar SubDAGs para procesos autÃ³nomos y muy pesados.
3. Configurar retries y timeout para tareas crÃ­ticas.
4. Versionar los DAGs con Git y validar cambios con CI/CD.
5. Mantener tests unitarios y de integraciÃ³n para detectar errores temprano.

---

## ðŸ“š Referencias

* [Airflow Documentation](https://airflow.apache.org/docs/)
* [Airflow TaskGroup vs SubDAG](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#taskgroups)
* [pytest fixtures](https://docs.pytest.org/en/stable/fixture.html)
* [GitHub Actions for Python](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python)


